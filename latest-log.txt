===== Application Startup at 2026-02-12 01:36:25 =====

2026-02-12 01:36:30,605 CRIT Supervisor is running as root.  Privileges were not dropped because no user is specified in the config file.  If you intend to run as root, you can set user=root in the config file to avoid this message.
2026-02-12 01:36:30,606 INFO supervisord started with pid 1
2026-02-12 01:36:31,610 INFO spawned: 'vllm_llm_service' with pid 51
2026-02-12 01:36:31,612 INFO spawned: 'backend' with pid 52
2026-02-12 01:36:31,613 INFO spawned: 'nextjs' with pid 53
2026-02-12 01:36:31,615 INFO spawned: 'nginx' with pid 54
===================================
Waiting for vLLM server to be ready...
===================================
Polling vLLM instance (timeout: 150s)
  - vLLM: http://localhost:8000/v1/models
[Attempt 1/30] Checking vLLM health...
=====================================
Waiting for all services to be ready...
=====================================

Checking vLLM at http://localhost:8000/v1/models (timeout: 150s)
  vLLM (8000): HTTP 000000
  Waiting 5s...
  [Attempt 1/30] vLLM not ready (HTTP 000000), waiting 5s...

> exaid-frontend@0.1.0 start
> next start

   ▲ Next.js 16.0.7
   - Local:         http://localhost:3000
   - Network:       http://10.29.92.103:3000

 ✓ Starting...
 ✓ Ready in 245ms
2026-02-12 01:36:33,225 INFO success: backend entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2026-02-12 01:36:33,225 INFO success: nextjs entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2026-02-12 01:36:33,225 INFO success: nginx entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
(APIServer pid=51) INFO 02-12 01:36:36 [utils.py:325] 
(APIServer pid=51) INFO 02-12 01:36:36 [utils.py:325]        █     █     █▄   ▄█
(APIServer pid=51) INFO 02-12 01:36:36 [utils.py:325]  ▄▄ ▄█ █     █     █ ▀▄▀ █  version 0.15.1
(APIServer pid=51) INFO 02-12 01:36:36 [utils.py:325]   █▄█▀ █     █     █     █  model   unsloth/medgemma-27b-text-it-unsloth-bnb-4bit
(APIServer pid=51) INFO 02-12 01:36:36 [utils.py:325]    ▀▀  ▀▀▀▀▀ ▀▀▀▀▀ ▀     ▀
(APIServer pid=51) INFO 02-12 01:36:36 [utils.py:325] 
(APIServer pid=51) INFO 02-12 01:36:36 [utils.py:261] non-default args: {'host': '127.0.0.1', 'model': 'unsloth/medgemma-27b-text-it-unsloth-bnb-4bit', 'max_model_len': 16384, 'quantization': 'bitsandbytes', 'enforce_eager': True, 'gpu_memory_utilization': 0.85, 'max_num_seqs': 32}
[Attempt 2/30] Checking vLLM health...
  vLLM (8000): HTTP 000000
  Waiting 5s...
  [Attempt 2/30] vLLM not ready (HTTP 000000), waiting 5s...
[Attempt 3/30] Checking vLLM health...
  vLLM (8000): HTTP 000000
  Waiting 5s...
  [Attempt 3/30] vLLM not ready (HTTP 000000), waiting 5s...
(APIServer pid=51) INFO 02-12 01:36:44 [model.py:541] Resolved architecture: Gemma3ForCausalLM
(APIServer pid=51) INFO 02-12 01:36:44 [model.py:1561] Using max model len 16384
(APIServer pid=51) INFO 02-12 01:36:44 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
(APIServer pid=51) INFO 02-12 01:36:44 [vllm.py:624] Asynchronous scheduling is enabled.
(APIServer pid=51) WARNING 02-12 01:36:44 [vllm.py:662] Enforce eager set, overriding optimization level to -O0
(APIServer pid=51) INFO 02-12 01:36:44 [vllm.py:762] Cudagraph is disabled under eager mode
[Attempt 4/30] Checking vLLM health...
  vLLM (8000): HTTP 000000
  Waiting 5s...
  [Attempt 4/30] vLLM not ready (HTTP 000000), waiting 5s...
/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[Attempt 5/30] Checking vLLM health...
  vLLM (8000): HTTP 000000
  Waiting 5s...
  [Attempt 5/30] vLLM not ready (HTTP 000000), waiting 5s...
(EngineCore_DP0 pid=151) INFO 02-12 01:36:53 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='unsloth/medgemma-27b-text-it-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/medgemma-27b-text-it-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=unsloth/medgemma-27b-text-it-unsloth-bnb-4bit, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
(EngineCore_DP0 pid=151) INFO 02-12 01:36:53 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.29.92.103:58755 backend=nccl
(EngineCore_DP0 pid=151) INFO 02-12 01:36:53 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
(EngineCore_DP0 pid=151) INFO 02-12 01:36:54 [gpu_model_runner.py:4033] Starting to load model unsloth/medgemma-27b-text-it-unsloth-bnb-4bit...
[Attempt 6/30] Checking vLLM health...
  vLLM (8000): HTTP 000000
  Waiting 5s...
  [Attempt 6/30] vLLM not ready (HTTP 000000), waiting 5s...
(EngineCore_DP0 pid=151) /opt/conda/lib/python3.10/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
(EngineCore_DP0 pid=151) We recommend installing via `pip install torch-c-dlpack-ext`
(EngineCore_DP0 pid=151)   warnings.warn(
(EngineCore_DP0 pid=151) INFO 02-12 01:36:57 [cuda.py:364] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=151) INFO 02-12 01:36:57 [bitsandbytes_loader.py:786] Loading weights with BitsAndBytes quantization. May take a while ...
[Attempt 7/30] Checking vLLM health...
  vLLM (8000): HTTP 000000
  Waiting 5s...
  [Attempt 7/30] vLLM not ready (HTTP 000000), waiting 5s...
[Attempt 8/30] Checking vLLM health...
  [Attempt 8/30] vLLM not ready (HTTP 000000), waiting 5s...
  vLLM (8000): HTTP 000000
  Waiting 5s...
[Attempt 9/30] Checking vLLM health...
  [Attempt 9/30] vLLM not ready (HTTP 000000), waiting 5s...
  vLLM (8000): HTTP 000000
  Waiting 5s...
[Attempt 10/30] Checking vLLM health...
  [Attempt 10/30] vLLM not ready (HTTP 000000), waiting 5s...
  vLLM (8000): HTTP 000000
  Waiting 5s...
[Attempt 11/30] Checking vLLM health...
  vLLM (8000): HTTP 000000
  Waiting 5s...
  [Attempt 11/30] vLLM not ready (HTTP 000000), waiting 5s...
[Attempt 12/30] Checking vLLM health...
  vLLM (8000): HTTP 000000
  Waiting 5s...
  [Attempt 12/30] vLLM not ready (HTTP 000000), waiting 5s...
[Attempt 13/30] Checking vLLM health...
2026-02-12 01:37:31,735 INFO success: vllm_llm_service entered RUNNING state, process has stayed up for > than 60 seconds (startsecs)
  vLLM (8000): HTTP 000000
  Waiting 5s...
  [Attempt 13/30] vLLM not ready (HTTP 000000), waiting 5s...
(EngineCore_DP0 pid=151) INFO 02-12 01:37:34 [weight_utils.py:527] Time spent downloading weights for unsloth/medgemma-27b-text-it-unsloth-bnb-4bit: 37.062558 seconds
(EngineCore_DP0 pid=151) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=151) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:00<00:00, 51.19it/s]
(EngineCore_DP0 pid=151) 
(EngineCore_DP0 pid=151) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=151) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  5.33it/s]
(EngineCore_DP0 pid=151) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.54it/s]
(EngineCore_DP0 pid=151) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.27it/s]
(EngineCore_DP0 pid=151) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.04it/s]
(EngineCore_DP0 pid=151) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.24it/s]
(EngineCore_DP0 pid=151) 
[Attempt 14/30] Checking vLLM health...
  vLLM (8000): HTTP 000000
  Waiting 5s...
  [Attempt 14/30] vLLM not ready (HTTP 000000), waiting 5s...
(EngineCore_DP0 pid=151) INFO 02-12 01:37:37 [gpu_model_runner.py:4130] Model loading took 16.91 GiB memory and 42.050194 seconds
[Attempt 15/30] Checking vLLM health...
  vLLM (8000): HTTP 000000
  Waiting 5s...
  [Attempt 15/30] vLLM not ready (HTTP 000000), waiting 5s...
(EngineCore_DP0 pid=151) INFO 02-12 01:37:43 [gpu_worker.py:356] Available KV cache memory: 20.19 GiB
(EngineCore_DP0 pid=151) WARNING 02-12 01:37:43 [kv_cache_utils.py:1047] Add 8 padding layers, may waste at most 15.38% KV cache memory
(EngineCore_DP0 pid=151) INFO 02-12 01:37:43 [kv_cache_utils.py:1307] GPU KV cache size: 37,808 tokens
(EngineCore_DP0 pid=151) INFO 02-12 01:37:43 [kv_cache_utils.py:1312] Maximum concurrency for 16,384 tokens per request: 7.58x
(EngineCore_DP0 pid=151) INFO 02-12 01:37:44 [core.py:272] init engine (profile, create kv cache, warmup model) took 6.82 seconds
(EngineCore_DP0 pid=151) INFO 02-12 01:37:45 [vllm.py:624] Asynchronous scheduling is enabled.
(EngineCore_DP0 pid=151) WARNING 02-12 01:37:45 [vllm.py:669] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.
(EngineCore_DP0 pid=151) INFO 02-12 01:37:45 [vllm.py:762] Cudagraph is disabled under eager mode
(APIServer pid=51) INFO 02-12 01:37:46 [api_server.py:665] Supported tasks: ['generate']
(APIServer pid=51) WARNING 02-12 01:37:46 [model.py:1371] Default vLLM sampling parameters have been overridden by the model's `generation_config.json`: `{'top_k': 64, 'top_p': 0.95}`. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
(APIServer pid=51) INFO 02-12 01:37:46 [serving.py:177] Warming up chat template processing...
[Attempt 16/30] Checking vLLM health...
  vLLM (8000): HTTP 000000
  Waiting 5s...
  [Attempt 16/30] vLLM not ready (HTTP 000000), waiting 5s...
(APIServer pid=51) INFO 02-12 01:37:47 [hf.py:310] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.
(APIServer pid=51) INFO 02-12 01:37:47 [serving.py:212] Chat template warmup completed in 1196.1ms
(APIServer pid=51) INFO 02-12 01:37:47 [api_server.py:946] Starting vLLM API server 0 on http://127.0.0.1:8000
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:38] Available routes are:
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /docs, Methods: GET, HEAD
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /tokenize, Methods: POST
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /detokenize, Methods: POST
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /pause, Methods: POST
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /resume, Methods: POST
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /is_paused, Methods: GET
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /metrics, Methods: GET
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /health, Methods: GET
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /v1/responses, Methods: POST
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /v1/completions, Methods: POST
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /v1/completions/render, Methods: POST
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /v1/messages, Methods: POST
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /v1/models, Methods: GET
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /load, Methods: GET
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /version, Methods: GET
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /ping, Methods: GET
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /ping, Methods: POST
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /invocations, Methods: POST
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /classify, Methods: POST
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /v1/embeddings, Methods: POST
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /score, Methods: POST
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /v1/score, Methods: POST
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /rerank, Methods: POST
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /v1/rerank, Methods: POST
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /v2/rerank, Methods: POST
(APIServer pid=51) INFO 02-12 01:37:47 [launcher.py:46] Route: /pooling, Methods: POST
(APIServer pid=51) INFO:     Started server process [51]
(APIServer pid=51) INFO:     Waiting for application startup.
(APIServer pid=51) INFO:     Application startup complete.
[Attempt 17/30] Checking vLLM health...
(APIServer pid=51) INFO:     127.0.0.1:51632 - "GET /v1/models HTTP/1.1" 200 OK
(APIServer pid=51) INFO:     127.0.0.1:51630 - "GET /v1/models HTTP/1.1" 200 OK
✓ vLLM is ready! (HTTP 200)

Checking Backend at http://localhost:8000/health (timeout: 150s)
✓ vLLM server is ready! Starting FastAPI backend...
(APIServer pid=51) INFO:     127.0.0.1:51640 - "GET /health HTTP/1.1" 200 OK
✓ Backend is ready! (HTTP 200)

Checking Next.js at http://localhost:3000 (timeout: 150s)
✓ Next.js is ready! (HTTP 200)

=====================================
✓ All services are ready!
=====================================
Starting nginx...

/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
INFO:     Started server process [52]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)
INFO:     10.16.6.60:0 - "WebSocket /ws" [accepted]
INFO:     connection open
INFO:     10.16.21.217:0 - "GET /api/traces HTTP/1.1" 200 OK
/app/infra/llm_registry.py:138: UserWarning: Parameters {'max_tokens'} should be specified explicitly. Instead they were passed in as part of `model_kwargs` parameter.
  return _registry.get_llm(role)
/app/infra/llm_registry.py:138: UserWarning: Parameters {'max_tokens', 'frequency_penalty'} should be specified explicitly. Instead they were passed in as part of `model_kwargs` parameter.
  return _registry.get_llm(role)
[EXAIM DEBUG] Calling _process_chunk for doctor0, flush_reason=boundary_cue
(APIServer pid=51) INFO:     127.0.0.1:41596 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [doctor0]: State=SAME_TOPIC_CONTINUING | Complete=True | Relevant=True | Novel=True | Trigger=True (path=A)
DEBUG [doctor0] Rationale: New leading diagnosis and differential.
(APIServer pid=51) INFO 02-12 01:38:17 [loggers.py:257] Engine 000: Avg prompt throughput: 194.5 tokens/s, Avg generation throughput: 4.8 tokens/s, Running: 0 reqs, Waiting: 1 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[EXAIM DEBUG] Calling _process_chunk for doctor0, flush_reason=boundary_cue
(APIServer pid=51) INFO 02-12 01:38:27 [loggers.py:257] Engine 000: Avg prompt throughput: 464.2 tokens/s, Avg generation throughput: 11.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.5%, Prefix cache hit rate: 26.7%
(EngineCore_DP0 pid=151) /opt/conda/lib/python3.10/site-packages/xgrammar/kernels/apply_token_bitmask_inplace_triton.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
(EngineCore_DP0 pid=151)   indices_cpu = torch.tensor(indices, dtype=torch.int32)
(APIServer pid=51) INFO:     127.0.0.1:45004 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [doctor0]: State=SAME_TOPIC_CONTINUING | Complete=False | Relevant=False | Novel=False | Trigger=False (path=-)
DEBUG [doctor0] Rationale: List of differential diagnoses, not a complete unit.
[EXAIM DEBUG] Calling _process_chunk for doctor0, flush_reason=boundary_cue
(APIServer pid=51) INFO 02-12 01:38:37 [loggers.py:257] Engine 000: Avg prompt throughput: 207.4 tokens/s, Avg generation throughput: 11.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.6%, Prefix cache hit rate: 40.5%
(EngineCore_DP0 pid=151) /opt/conda/lib/python3.10/site-packages/xgrammar/kernels/apply_token_bitmask_inplace_triton.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
(EngineCore_DP0 pid=151)   indices_cpu = torch.tensor(indices, dtype=torch.int32)
(APIServer pid=51) INFO:     127.0.0.1:45004 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [doctor0]: State=SAME_TOPIC_CONTINUING | Complete=True | Relevant=True | Novel=True | Trigger=True (path=A)
DEBUG [doctor0] Rationale: New, concrete plan items.
[EXAIM DEBUG] Calling _process_chunk for doctor1, flush_reason=boundary_cue
(APIServer pid=51) INFO 02-12 01:38:47 [loggers.py:257] Engine 000: Avg prompt throughput: 479.8 tokens/s, Avg generation throughput: 14.2 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.6%, Prefix cache hit rate: 57.8%
(APIServer pid=51) INFO:     127.0.0.1:39526 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [doctor1]: State=SAME_TOPIC_CONTINUING | Complete=True | Relevant=True | Novel=True | Trigger=True (path=A)
DEBUG [doctor1] Rationale: New leading diagnosis and differential list.
[EXAIM DEBUG] Calling _process_chunk for doctor1, flush_reason=boundary_cue
(APIServer pid=51) INFO 02-12 01:38:57 [loggers.py:257] Engine 000: Avg prompt throughput: 467.2 tokens/s, Avg generation throughput: 19.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.8%, Prefix cache hit rate: 66.5%
(EngineCore_DP0 pid=151) /opt/conda/lib/python3.10/site-packages/xgrammar/kernels/apply_token_bitmask_inplace_triton.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
(EngineCore_DP0 pid=151)   indices_cpu = torch.tensor(indices, dtype=torch.int32)
(APIServer pid=51) INFO:     127.0.0.1:53510 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [doctor1]: State=SAME_TOPIC_CONTINUING | Complete=True | Relevant=False | Novel=False | Trigger=False (path=-)
DEBUG [doctor1] Rationale: List of differential diagnoses, no action.
[EXAIM DEBUG] Calling _process_chunk for doctor1, flush_reason=boundary_cue
(APIServer pid=51) INFO 02-12 01:39:07 [loggers.py:257] Engine 000: Avg prompt throughput: 209.7 tokens/s, Avg generation throughput: 23.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.2%, Prefix cache hit rate: 68.3%
(APIServer pid=51) INFO:     127.0.0.1:53510 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [doctor1]: State=SAME_TOPIC_CONTINUING | Complete=True | Relevant=True | Novel=True | Trigger=True (path=A)
DEBUG [doctor1] Rationale: New, concrete plan/action items.
(APIServer pid=51) INFO 02-12 01:39:17 [loggers.py:257] Engine 000: Avg prompt throughput: 286.7 tokens/s, Avg generation throughput: 23.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.4%, Prefix cache hit rate: 70.7%
[EXAIM DEBUG] Calling _process_chunk for doctor2, flush_reason=boundary_cue
(APIServer pid=51) INFO 02-12 01:39:27 [loggers.py:257] Engine 000: Avg prompt throughput: 203.1 tokens/s, Avg generation throughput: 29.1 tokens/s, Running: 5 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.1%, Prefix cache hit rate: 72.0%
(APIServer pid=51) INFO:     127.0.0.1:44130 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [doctor2]: State=SAME_TOPIC_CONTINUING | Complete=True | Relevant=False | Novel=False | Trigger=False (path=-)
DEBUG [doctor2] Rationale: Reiterates leading diagnosis and differential, no new action.
[EXAIM DEBUG] Calling _process_chunk for doctor2, flush_reason=boundary_cue
(APIServer pid=51) INFO 02-12 01:39:37 [loggers.py:257] Engine 000: Avg prompt throughput: 215.3 tokens/s, Avg generation throughput: 28.7 tokens/s, Running: 5 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.5%, Prefix cache hit rate: 73.0%
(APIServer pid=51) INFO:     127.0.0.1:44130 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [doctor2]: State=SAME_TOPIC_CONTINUING | Complete=True | Relevant=False | Novel=False | Trigger=False (path=-)
DEBUG [doctor2] Rationale: Elaboration on differential diagnoses, not a new action or stance.
[EXAIM DEBUG] Calling _process_chunk for doctor2, flush_reason=boundary_cue
(APIServer pid=51) INFO 02-12 01:39:47 [loggers.py:257] Engine 000: Avg prompt throughput: 228.7 tokens/s, Avg generation throughput: 29.1 tokens/s, Running: 5 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.1%, Prefix cache hit rate: 74.1%
(APIServer pid=51) INFO:     127.0.0.1:44130 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [doctor2]: State=SAME_TOPIC_CONTINUING | Complete=True | Relevant=True | Novel=True | Trigger=True (path=A)
DEBUG [doctor2] Rationale: Adds a new differential and lists tests.
[EXAIM DEBUG] Calling _process_chunk for doctor2, flush_reason=boundary_cue
(APIServer pid=51) INFO 02-12 01:39:57 [loggers.py:257] Engine 000: Avg prompt throughput: 500.4 tokens/s, Avg generation throughput: 32.3 tokens/s, Running: 6 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.0%, Prefix cache hit rate: 75.8%
(APIServer pid=51) INFO:     127.0.0.1:36052 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [doctor2]: State=SAME_TOPIC_CONTINUING | Complete=False | Relevant=False | Novel=False | Trigger=False (path=-)
DEBUG [doctor2] Rationale: List of tests, not a complete unit.
[EXAIM DEBUG] Calling _process_chunk for supervisor, flush_reason=boundary_cue
(APIServer pid=51) INFO 02-12 01:40:07 [loggers.py:257] Engine 000: Avg prompt throughput: 210.3 tokens/s, Avg generation throughput: 33.2 tokens/s, Running: 6 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.8%, Prefix cache hit rate: 76.2%
(APIServer pid=51) INFO:     127.0.0.1:36052 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [supervisor]: State=SAME_TOPIC_CONTINUING | Complete=True | Relevant=False | Novel=False | Trigger=False (path=-)
DEBUG [supervisor] Rationale: Summarizes consensus, no new action/finding.
[EXAIM DEBUG] Calling _process_chunk for supervisor, flush_reason=boundary_cue
[EXAIM Background] Summarizer timed out after 120s (trigger_id=d1daa8ee-ae87-467a-9654-a17cdc9bdc32)
(APIServer pid=51) INFO 02-12 01:40:17 [loggers.py:257] Engine 000: Avg prompt throughput: 223.5 tokens/s, Avg generation throughput: 34.2 tokens/s, Running: 5 reqs, Waiting: 0 reqs, GPU KV cache usage: 13.0%, Prefix cache hit rate: 76.7%
(APIServer pid=51) INFO:     127.0.0.1:36052 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [supervisor]: State=SAME_TOPIC_CONTINUING | Complete=True | Relevant=False | Novel=False | Trigger=False (path=-)
DEBUG [supervisor] Rationale: Elaboration on differentials, no new action/stance.
[EXAIM DEBUG] Calling _process_chunk for supervisor, flush_reason=boundary_cue
(APIServer pid=51) INFO 02-12 01:40:27 [loggers.py:257] Engine 000: Avg prompt throughput: 236.4 tokens/s, Avg generation throughput: 28.8 tokens/s, Running: 5 reqs, Waiting: 0 reqs, GPU KV cache usage: 13.5%, Prefix cache hit rate: 77.3%
(APIServer pid=51) INFO:     127.0.0.1:36052 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [supervisor]: State=SAME_TOPIC_CONTINUING | Complete=True | Relevant=False | Novel=False | Trigger=False (path=-)
DEBUG [supervisor] Rationale: Summarizes previous points, no new action/stance.
(APIServer pid=51) INFO 02-12 01:40:37 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 29.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.8%, Prefix cache hit rate: 77.3%
[EXAIM DEBUG] Calling _process_chunk for supervisor, flush_reason=boundary_cue
[EXAIM Background] Summarizer timed out after 120s (trigger_id=e7c7957d-fc4c-487f-a84c-54fb8e1e1126)
(APIServer pid=51) INFO 02-12 01:40:47 [loggers.py:257] Engine 000: Avg prompt throughput: 251.6 tokens/s, Avg generation throughput: 23.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.8%, Prefix cache hit rate: 77.9%
(APIServer pid=51) INFO:     127.0.0.1:36052 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [supervisor]: State=SAME_TOPIC_CONTINUING | Complete=True | Relevant=False | Novel=False | Trigger=False (path=-)
DEBUG [supervisor] Rationale: Summarization of discussion points, not a new action/finding.
[EXAIM DEBUG] Calling _process_chunk for supervisor, flush_reason=boundary_cue
[EXAIM Background] Summarizer timed out after 120s (trigger_id=81ccac91-fbff-4b82-9a56-454700f25f26)
(APIServer pid=51) INFO 02-12 01:40:57 [loggers.py:257] Engine 000: Avg prompt throughput: 267.6 tokens/s, Avg generation throughput: 19.4 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.6%, Prefix cache hit rate: 78.4%
(APIServer pid=51) INFO:     127.0.0.1:36052 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [supervisor]: State=SAME_TOPIC_CONTINUING | Complete=True | Relevant=False | Novel=False | Trigger=False (path=-)
DEBUG [supervisor] Rationale: Summarization prompt, not actionable content.
[EXAIM DEBUG] Calling _process_chunk for doctor0, flush_reason=boundary_cue
(APIServer pid=51) INFO 02-12 01:41:07 [loggers.py:257] Engine 000: Avg prompt throughput: 282.5 tokens/s, Avg generation throughput: 15.2 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.8%, Prefix cache hit rate: 78.9%
(APIServer pid=51) INFO:     127.0.0.1:36052 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [doctor0]: State=SAME_TOPIC_CONTINUING | Complete=True | Relevant=False | Novel=False | Trigger=False (path=-)
DEBUG [doctor0] Rationale: Elaboration on a previously discussed differential diagnosis.
[EXAIM DEBUG] Calling _process_chunk for doctor0, flush_reason=boundary_cue
[EXAIM Background] Summarizer timed out after 120s (trigger_id=54365e2f-8581-4aa0-9bbd-047d03248a78)
(APIServer pid=51) INFO 02-12 01:41:17 [loggers.py:257] Engine 000: Avg prompt throughput: 296.5 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.2%, Prefix cache hit rate: 79.4%
(APIServer pid=51) INFO:     127.0.0.1:36052 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [doctor0]: State=SAME_TOPIC_CONTINUING | Complete=True | Relevant=False | Novel=False | Trigger=False (path=-)
DEBUG [doctor0] Rationale: Elaboration on previously discussed points (diet, family history).
[EXAIM DEBUG] Calling _process_chunk for doctor0, flush_reason=boundary_cue
(APIServer pid=51) INFO 02-12 01:41:27 [loggers.py:257] Engine 000: Avg prompt throughput: 307.6 tokens/s, Avg generation throughput: 11.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.2%, Prefix cache hit rate: 80.0%
(APIServer pid=51) INFO:     127.0.0.1:36052 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [doctor0]: State=SAME_TOPIC_CONTINUING | Complete=True | Relevant=True | Novel=True | Trigger=True (path=A)
DEBUG [doctor0] Rationale: Completes a unit on toxic exposures and suggests a family history assessment.
[EXAIM DEBUG] Calling _process_chunk for doctor0, flush_reason=boundary_cue
(APIServer pid=51) INFO 02-12 01:41:37 [loggers.py:257] Engine 000: Avg prompt throughput: 576.0 tokens/s, Avg generation throughput: 12.1 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.7%, Prefix cache hit rate: 79.5%
(APIServer pid=51) INFO:     127.0.0.1:58842 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [doctor0]: State=SAME_TOPIC_CONTINUING | Complete=True | Relevant=False | Novel=False | Trigger=False (path=-)
DEBUG [doctor0] Rationale: New plan items, but not specific enough to be actionable.
(APIServer pid=51) INFO 02-12 01:41:47 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 16.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.7%, Prefix cache hit rate: 79.5%
[EXAIM Background] Summarizer timed out after 120s (trigger_id=04af3341-ad72-418c-904d-ecdf5e0d9e6b)
[EXAIM DEBUG] Calling _process_chunk for doctor1, flush_reason=boundary_cue
(APIServer pid=51) INFO 02-12 01:41:57 [loggers.py:257] Engine 000: Avg prompt throughput: 206.4 tokens/s, Avg generation throughput: 11.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.0%, Prefix cache hit rate: 79.7%
(APIServer pid=51) INFO:     127.0.0.1:58842 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [doctor1]: State=SAME_TOPIC_CONTINUING | Complete=True | Relevant=False | Novel=False | Trigger=False (path=-)
DEBUG [doctor1] Rationale: Elaboration on existing points, no new action/stance.
[EXAIM DEBUG] Calling _process_chunk for doctor1, flush_reason=boundary_cue
(APIServer pid=51) INFO 02-12 01:42:07 [loggers.py:257] Engine 000: Avg prompt throughput: 219.3 tokens/s, Avg generation throughput: 11.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.1%, Prefix cache hit rate: 79.9%
(APIServer pid=51) INFO:     127.0.0.1:58842 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [doctor1]: State=SAME_TOPIC_CONTINUING | Complete=False | Relevant=False | Novel=False | Trigger=False (path=-)
DEBUG [doctor1] Rationale: Elaboration on previous points, not a new action.
[EXAIM DEBUG] Calling _process_chunk for doctor1, flush_reason=boundary_cue
(APIServer pid=51) INFO 02-12 01:42:17 [loggers.py:257] Engine 000: Avg prompt throughput: 231.3 tokens/s, Avg generation throughput: 11.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.2%, Prefix cache hit rate: 80.1%
(APIServer pid=51) INFO:     127.0.0.1:58842 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [doctor1]: State=SAME_TOPIC_CONTINUING | Complete=True | Relevant=False | Novel=False | Trigger=False (path=-)
DEBUG [doctor1] Rationale: Elaboration on previous points, no new action.
[EXAIM DEBUG] Calling _process_chunk for doctor1, flush_reason=boundary_cue
(APIServer pid=51) INFO 02-12 01:42:27 [loggers.py:257] Engine 000: Avg prompt throughput: 241.3 tokens/s, Avg generation throughput: 11.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.2%, Prefix cache hit rate: 80.4%
(APIServer pid=51) INFO:     127.0.0.1:58842 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [doctor1]: State=SAME_TOPIC_CONTINUING | Complete=True | Relevant=False | Novel=False | Trigger=False (path=-)
DEBUG [doctor1] Rationale: Repetitive, no new action/stance/finding.
[EXAIM DEBUG] Calling _process_chunk for supervisor, flush_reason=boundary_cue
(APIServer pid=51) INFO 02-12 01:42:37 [loggers.py:257] Engine 000: Avg prompt throughput: 262.5 tokens/s, Avg generation throughput: 10.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.3%, Prefix cache hit rate: 79.9%
(APIServer pid=51) INFO:     127.0.0.1:58842 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [supervisor]: State=SAME_TOPIC_CONTINUING | Complete=True | Relevant=False | Novel=False | Trigger=False (path=-)
DEBUG [supervisor] Rationale: Agreement and restatement of previous points, no new action.
[EXAIM DEBUG] Calling _process_chunk for supervisor, flush_reason=boundary_cue
(APIServer pid=51) INFO 02-12 01:42:47 [loggers.py:257] Engine 000: Avg prompt throughput: 274.5 tokens/s, Avg generation throughput: 11.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.4%, Prefix cache hit rate: 80.3%
(APIServer pid=51) INFO:     127.0.0.1:58842 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [supervisor]: State=SAME_TOPIC_CONTINUING | Complete=True | Relevant=False | Novel=False | Trigger=False (path=-)
DEBUG [supervisor] Rationale: Elaboration on previously discussed points (diet, toxins). No new action/stance.
[EXAIM DEBUG] Calling _process_chunk for supervisor, flush_reason=boundary_cue
(APIServer pid=51) INFO 02-12 01:42:57 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 11.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.0%, Prefix cache hit rate: 80.3%
(APIServer pid=51) INFO:     127.0.0.1:58842 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [supervisor]: State=SAME_TOPIC_CONTINUING | Complete=True | Relevant=True | Novel=True | Trigger=True (path=A)
DEBUG [supervisor] Rationale: Summarizes previous discussion and proposes concrete next steps.
(APIServer pid=51) INFO 02-12 01:43:07 [loggers.py:257] Engine 000: Avg prompt throughput: 648.3 tokens/s, Avg generation throughput: 11.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.8%, Prefix cache hit rate: 80.1%
[EXAIM DEBUG] Calling _process_chunk for supervisor, flush_reason=boundary_cue
(APIServer pid=51) INFO:     127.0.0.1:46806 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [supervisor]: State=SAME_TOPIC_CONTINUING | Complete=True | Relevant=True | Novel=True | Trigger=True (path=A)
DEBUG [supervisor] Rationale: Complete summary of diagnosis and plan.
(APIServer pid=51) INFO 02-12 01:43:17 [loggers.py:257] Engine 000: Avg prompt throughput: 198.4 tokens/s, Avg generation throughput: 16.9 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.8%, Prefix cache hit rate: 80.7%
[EXAIM DEBUG] Calling _process_chunk for supervisor, flush_reason=boundary_cue
(APIServer pid=51) INFO 02-12 01:43:27 [loggers.py:257] Engine 000: Avg prompt throughput: 467.2 tokens/s, Avg generation throughput: 22.7 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.5%, Prefix cache hit rate: 81.0%
(APIServer pid=51) INFO:     127.0.0.1:44540 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [supervisor]: State=SAME_TOPIC_CONTINUING | Complete=True | Relevant=False | Novel=False | Trigger=False (path=-)
DEBUG [supervisor] Rationale: Concluding statement, no new action/finding.
[EXAIM DEBUG] Calling _process_chunk for supervisor, flush_reason=boundary_cue
[EXAIM Background] Summarizer timed out after 120s (trigger_id=ea8c7ab7-93fe-4fe5-b137-8d45f7c97794)
(APIServer pid=51) INFO 02-12 01:43:37 [loggers.py:257] Engine 000: Avg prompt throughput: 208.9 tokens/s, Avg generation throughput: 19.3 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.5%, Prefix cache hit rate: 81.0%
(APIServer pid=51) INFO:     127.0.0.1:44540 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [supervisor]: State=SAME_TOPIC_CONTINUING | Complete=True | Relevant=True | Novel=True | Trigger=True (path=A)
DEBUG [supervisor] Rationale: Summarizes agreed-upon diagnosis and plan.
[EXAIM DEBUG] Calling _process_chunk for supervisor, flush_reason=boundary_cue
(APIServer pid=51) INFO 02-12 01:43:47 [loggers.py:257] Engine 000: Avg prompt throughput: 479.9 tokens/s, Avg generation throughput: 19.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.5%, Prefix cache hit rate: 81.4%
(APIServer pid=51) INFO:     127.0.0.1:47018 - "POST /v1/chat/completions HTTP/1.1" 200 OK
DEBUG [supervisor]: State=SAME_TOPIC_CONTINUING | Complete=False | Relevant=False | Novel=False | Trigger=False (path=-)
DEBUG [supervisor] Rationale: List of tests and exploration areas, not a single complete unit.
(APIServer pid=51) INFO 02-12 01:43:57 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 22.1 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.6%, Prefix cache hit rate: 81.4%
(APIServer pid=51) INFO 02-12 01:44:07 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 18.0 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.8%, Prefix cache hit rate: 81.4%
(APIServer pid=51) INFO 02-12 01:44:17 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 18.3 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.0%, Prefix cache hit rate: 81.4%
(APIServer pid=51) INFO 02-12 01:44:27 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 18.0 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.2%, Prefix cache hit rate: 81.4%
(APIServer pid=51) INFO 02-12 01:44:37 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 18.3 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.5%, Prefix cache hit rate: 81.4%
(APIServer pid=51) INFO 02-12 01:44:47 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 18.0 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.6%, Prefix cache hit rate: 81.4%
(APIServer pid=51) INFO 02-12 01:44:57 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 18.3 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.8%, Prefix cache hit rate: 81.4%
[EXAIM Background] Summarizer timed out after 120s (trigger_id=c4b59dd2-5d6c-4c19-83b6-daf75f2e0e05)
(APIServer pid=51) INFO 02-12 01:45:07 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.0%, Prefix cache hit rate: 81.4%
[EXAIM Background] Summarizer timed out after 120s (trigger_id=8a66fdf5-102f-484d-afa1-4a8e52f11659)
(APIServer pid=51) INFO 02-12 01:45:17 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 11.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 81.4%
(APIServer pid=51) INFO 02-12 01:45:27 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 81.4%
(APIServer pid=51) INFO 02-12 01:45:37 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 81.4%
[EXAIM Background] Summarizer timed out after 120s (trigger_id=16ac7616-0390-4d59-ac1c-9853c32077ac)
(APIServer pid=51) INFO 02-12 01:45:47 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 3.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 81.4%
(APIServer pid=51) INFO 02-12 01:45:57 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 81.4%
 