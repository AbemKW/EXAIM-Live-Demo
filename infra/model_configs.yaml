# LLM Model Configuration for EXAIM
# Environment variables can override these defaults.

# MAS (Multi-Agent System)
# Used by: CDSS demo agents (cardiology, internal_medicine, laboratory, orchestrator, radiology)
# Purpose: Generate reasoning traces from specialized agents
mas:
  provider: google
  model:  gemini-2.5-flash-lite
  streaming: true
  # Env overrides: MAS_LLM_PROVIDER, MAS_LLM_MODEL

# Summarizer - Clinical Summary Generation
# Used by: SummarizerAgent
# Purpose: Generate structured clinical summaries from agent traces
# vLLM server on port 8001 (internal only, not exposed publicly)
summarizer:
  provider: openai
  model: google/medgemma-1.5-4b-it
  streaming: true
  # Env overrides: SUMMARIZER_LLM_PROVIDER, SUMMARIZER_LLM_MODEL, OPENAI_BASE_URL=http://localhost:8001/v1

# Buffer Agent - Trace Trigger Decision
# Used by: BufferAgent
# Purpose: Decide when to trigger summarization based on trace completeness and novelty
# Uses guided JSON generation with BufferAnalysis schema for structured output
# vLLM server on port 8001 with Outlines backend for guaranteed JSON format
buffer_agent:
  provider: openai
  model: google/medgemma-1.5-4b-it
  streaming: true
  # Env overrides: BUFFER_AGENT_LLM_PROVIDER, BUFFER_AGENT_LLM_MODEL, OPENAI_BASE_URL=http://localhost:8001/v1