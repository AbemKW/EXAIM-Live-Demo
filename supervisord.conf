[supervisord]
nodaemon=true
logfile=/tmp/supervisord.log
pidfile=/tmp/supervisord.pid

[program:vllm_summarizer]
command=python -m vllm.entrypoints.openai.api_server --model unsloth/medgemma-4b-it-unsloth-bnb-4bit --port 8001 --host 127.0.0.1 --gpu-memory-utilization 0.25 --quantization bitsandbytes --max-model-len 4200 --max-num-seqs 32 --enforce-eager --uvicorn-log-level info
directory=/app
autostart=true
autorestart=true
startretries=3
startsecs=60
redirect_stderr=true
stdout_logfile=/dev/stdout
stdout_logfile_maxbytes=0
priority=100

[program:vllm_buffer]
command=python -m vllm.entrypoints.openai.api_server --model unsloth/medgemma-4b-it-unsloth-bnb-4bit --port 8002 --host 127.0.0.1 --gpu-memory-utilization 0.25 --quantization bitsandbytes --max-model-len 4200 --max-num-seqs 16 --enforce-eager --uvicorn-log-level info
directory=/app
autostart=true
autorestart=true
startretries=3
startsecs=60
redirect_stderr=true
stdout_logfile=/dev/stdout
stdout_logfile_maxbytes=0
priority=99

[program:backend]
command=bash /app/start_backend.sh
directory=/app
autostart=true
autorestart=true
startretries=3
redirect_stderr=true
stdout_logfile=/dev/stdout
stdout_logfile_maxbytes=0
priority=200
environment=PYTHONUNBUFFERED=1

[program:nextjs]
command=npm start
directory=/app/demos/frontend
autostart=true
autorestart=true
redirect_stderr=true
stdout_logfile=/dev/stdout
stdout_logfile_maxbytes=0
environment=NODE_ENV=production,PORT=3000

[program:nginx]
command=bash -c "/app/wait-for-all-services.sh && exec /usr/sbin/nginx -g 'daemon off;'"
autostart=true
autorestart=true
redirect_stderr=true
stdout_logfile=/dev/stdout
stdout_logfile_maxbytes=0
priority=999
